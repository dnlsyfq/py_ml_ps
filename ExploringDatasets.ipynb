{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy as sp \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.21.3\n1.17.3\n0.25.2\n1.3.1\n0.25.2\n"
    }
   ],
   "source": [
    "print(sklearn.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(sp.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer_dataset = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry \n        - fractal dimension (&quot;coastline approximation&quot; - 1)\n\n        The mean, standard error, and &quot;worst&quot; or largest (mean of the three\n        largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n        13 is Radius SE, field 23 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, &quot;Decision Tree\nConstruction Via Linear Programming.&quot; Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: &quot;Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets&quot;,\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171.\n"
    }
   ],
   "source": [
    "print(breast_cancer_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "breast_cancer_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;,\n       &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;,\n       &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;,\n       &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;,\n       &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;,\n       &#39;concave points error&#39;, &#39;symmetry error&#39;,\n       &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;,\n       &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;,\n       &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;,\n       &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "breast_cancer_dataset.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(569, 30)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "breast_cancer_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "breast_cancer_dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(569,)"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "breast_cancer_dataset.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame(\n",
    "    breast_cancer_dataset.data,\n",
    "    columns=breast_cancer_dataset.feature_names\n",
    ")\n",
    "\n",
    "df_target = pd.DataFrame(\n",
    "    breast_cancer_dataset.target,\n",
    "    columns=['cancer']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_features,df_target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0        17.99         10.38          122.80     1001.0          0.11840   \n1        20.57         17.77          132.90     1326.0          0.08474   \n2        19.69         21.25          130.00     1203.0          0.10960   \n3        11.42         20.38           77.58      386.1          0.14250   \n4        20.29         14.34          135.10     1297.0          0.10030   \n\n   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0           0.27760          0.3001              0.14710         0.2419   \n1           0.07864          0.0869              0.07017         0.1812   \n2           0.15990          0.1974              0.12790         0.2069   \n3           0.28390          0.2414              0.10520         0.2597   \n4           0.13280          0.1980              0.10430         0.1809   \n\n   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                 0.07871  ...          17.33           184.60      2019.0   \n1                 0.05667  ...          23.41           158.80      1956.0   \n2                 0.05999  ...          25.53           152.50      1709.0   \n3                 0.09744  ...          26.50            98.87       567.7   \n4                 0.05883  ...          16.67           152.20      1575.0   \n\n   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n0            0.1622             0.6656           0.7119                0.2654   \n1            0.1238             0.1866           0.2416                0.1860   \n2            0.1444             0.4245           0.4504                0.2430   \n3            0.2098             0.8663           0.6869                0.2575   \n4            0.1374             0.2050           0.4000                0.1625   \n\n   worst symmetry  worst fractal dimension  cancer  \n0          0.4601                  0.11890       0  \n1          0.2750                  0.08902       0  \n2          0.3613                  0.08758       0  \n3          0.6638                  0.17300       0  \n4          0.2364                  0.07678       0  \n\n[5 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>cancer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(569, 31)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;feature_names&#39;, &#39;DESCR&#39;, &#39;filename&#39;])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "boston_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".. _boston_dataset:\n\nBoston house prices dataset\n---------------------------\n\n**Data Set Characteristics:**  \n\n    :Number of Instances: 506 \n\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000&#39;s\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. &#39;Hedonic\nprices and the demand for clean air&#39;, J. Environ. Economics &amp; Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics\n...&#39;, Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n     \n.. topic:: References\n\n   - Belsley, Kuh &amp; Welsch, &#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity&#39;, Wiley, 1980. 244-261.\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n\n"
    }
   ],
   "source": [
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading 20news dataset. This may take a few minutes.\nDownloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
    }
   ],
   "source": [
    "fetch_20_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys([&#39;data&#39;, &#39;filenames&#39;, &#39;target_names&#39;, &#39;target&#39;, &#39;DESCR&#39;])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "fetch_20_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".. _20newsgroups_dataset:\n\nThe 20 newsgroups text dataset\n------------------------------\n\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\n20 topics split in two subsets: one for training (or development)\nand the other one for testing (or for performance evaluation). The split\nbetween the train and test set is based upon a messages posted before\nand after a specific date.\n\nThis module contains two loaders. The first one,\n:func:`sklearn.datasets.fetch_20newsgroups`,\nreturns a list of the raw texts that can be fed to text feature\nextractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\nwith custom parameters so as to extract feature vectors.\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\nreturns ready-to-use features, i.e., it is not necessary to use a feature\nextractor.\n\n**Data Set Characteristics:**\n\n    =================   ==========\n    Classes                     20\n    Samples total            18846\n    Dimensionality               1\n    Features                  text\n    =================   ==========\n\nUsage\n~~~~~\n\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\nfetching / caching functions that downloads the data archive from\nthe original `20 newsgroups website`_, extracts the archive contents\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\n:func:`sklearn.datasets.load_files` on either the training or\ntesting set folder, or both of them::\n\n  &gt;&gt;&gt; from sklearn.datasets import fetch_20newsgroups\n  &gt;&gt;&gt; newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;)\n\n  &gt;&gt;&gt; from pprint import pprint\n  &gt;&gt;&gt; pprint(list(newsgroups_train.target_names))\n  [&#39;alt.atheism&#39;,\n   &#39;comp.graphics&#39;,\n   &#39;comp.os.ms-windows.misc&#39;,\n   &#39;comp.sys.ibm.pc.hardware&#39;,\n   &#39;comp.sys.mac.hardware&#39;,\n   &#39;comp.windows.x&#39;,\n   &#39;misc.forsale&#39;,\n   &#39;rec.autos&#39;,\n   &#39;rec.motorcycles&#39;,\n   &#39;rec.sport.baseball&#39;,\n   &#39;rec.sport.hockey&#39;,\n   &#39;sci.crypt&#39;,\n   &#39;sci.electronics&#39;,\n   &#39;sci.med&#39;,\n   &#39;sci.space&#39;,\n   &#39;soc.religion.christian&#39;,\n   &#39;talk.politics.guns&#39;,\n   &#39;talk.politics.mideast&#39;,\n   &#39;talk.politics.misc&#39;,\n   &#39;talk.religion.misc&#39;]\n\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\nattribute is the integer index of the category::\n\n  &gt;&gt;&gt; newsgroups_train.filenames.shape\n  (11314,)\n  &gt;&gt;&gt; newsgroups_train.target.shape\n  (11314,)\n  &gt;&gt;&gt; newsgroups_train.target[:10]\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n\nIt is possible to load only a sub-selection of the categories by passing the\nlist of the categories to load to the\n:func:`sklearn.datasets.fetch_20newsgroups` function::\n\n  &gt;&gt;&gt; cats = [&#39;alt.atheism&#39;, &#39;sci.space&#39;]\n  &gt;&gt;&gt; newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;, categories=cats)\n\n  &gt;&gt;&gt; list(newsgroups_train.target_names)\n  [&#39;alt.atheism&#39;, &#39;sci.space&#39;]\n  &gt;&gt;&gt; newsgroups_train.filenames.shape\n  (1073,)\n  &gt;&gt;&gt; newsgroups_train.target.shape\n  (1073,)\n  &gt;&gt;&gt; newsgroups_train.target[:10]\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n\nConverting text to vectors\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn order to feed predictive or clustering models with the text data,\none first need to turn the text into vectors of numerical values suitable\nfor statistical analysis. This can be achieved with the utilities of the\n``sklearn.feature_extraction.text`` as demonstrated in the following\nexample that extract `TF-IDF`_ vectors of unigram tokens\nfrom a subset of 20news::\n\n  &gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n  &gt;&gt;&gt; categories = [&#39;alt.atheism&#39;, &#39;talk.religion.misc&#39;,\n  ...               &#39;comp.graphics&#39;, &#39;sci.space&#39;]\n  &gt;&gt;&gt; newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;,\n  ...                                       categories=categories)\n  &gt;&gt;&gt; vectorizer = TfidfVectorizer()\n  &gt;&gt;&gt; vectors = vectorizer.fit_transform(newsgroups_train.data)\n  &gt;&gt;&gt; vectors.shape\n  (2034, 34118)\n\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\ncomponents by sample in a more than 30000-dimensional space\n(less than .5% non-zero features)::\n\n  &gt;&gt;&gt; vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n  159.01327...\n\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \nreturns ready-to-use token counts features instead of file names.\n\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n\n\nFiltering text for more realistic training\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is easy for a classifier to overfit on particular things that appear in the\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\nhigh F-scores, but their results would not generalize to other documents that\naren&#39;t from this window of time.\n\nFor example, let&#39;s look at the results of a multinomial Naive Bayes classifier,\nwhich is fast to train and achieves a decent F-score::\n\n  &gt;&gt;&gt; from sklearn.naive_bayes import MultinomialNB\n  &gt;&gt;&gt; from sklearn import metrics\n  &gt;&gt;&gt; newsgroups_test = fetch_20newsgroups(subset=&#39;test&#39;,\n  ...                                      categories=categories)\n  &gt;&gt;&gt; vectors_test = vectorizer.transform(newsgroups_test.data)\n  &gt;&gt;&gt; clf = MultinomialNB(alpha=.01)\n  &gt;&gt;&gt; clf.fit(vectors, newsgroups_train.target)\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n  &gt;&gt;&gt; pred = clf.predict(vectors_test)\n  &gt;&gt;&gt; metrics.f1_score(newsgroups_test.target, pred, average=&#39;macro&#39;)  # doctest: +ELLIPSIS\n  0.88213...\n\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\nthe training and test data, instead of segmenting by time, and in that case\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\nyet of what&#39;s going on inside this classifier?)\n\nLet&#39;s take a look at what the most informative features are:\n\n  &gt;&gt;&gt; import numpy as np\n  &gt;&gt;&gt; def show_top10(classifier, vectorizer, categories):\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n  ...     for i, category in enumerate(categories):\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n  ...         print(&quot;%s: %s&quot; % (category, &quot; &quot;.join(feature_names[top10])))\n  ...\n  &gt;&gt;&gt; show_top10(clf, vectorizer, newsgroups_train.target_names)\n  alt.atheism: edu it and in you that is of to the\n  comp.graphics: edu in graphics it is for and of to the\n  sci.space: edu it that is in and space to of the\n  talk.religion.misc: not it you in is that and to of the\n\n\nYou can now see many things that these features have overfit to:\n\n- Almost every group is distinguished by whether headers such as\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n- Another significant feature involves whether the sender is affiliated with\n  a university, as indicated either by their headers or their signature.\n- The word &quot;article&quot; is a significant feature, based on how often people quote\n  previous posts like this: &quot;In article [article ID], [name] &lt;[e-mail address]&gt;\n  wrote:&quot;\n- Other features match the names and e-mail addresses of particular people who\n  were posting at the time.\n\nWith such an abundance of clues that distinguish newsgroups, the classifiers\nbarely have to identify topics from text at all, and they all perform at the\nsame high level.\n\nFor this reason, the functions that load 20 Newsgroups data provide a\nparameter called **remove**, telling it what kinds of information to strip out\nof each file. **remove** should be a tuple containing any subset of\n``(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)``, telling it to remove headers, signature\nblocks, and quotation blocks respectively.\n\n  &gt;&gt;&gt; newsgroups_test = fetch_20newsgroups(subset=&#39;test&#39;,\n  ...                                      remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;),\n  ...                                      categories=categories)\n  &gt;&gt;&gt; vectors_test = vectorizer.transform(newsgroups_test.data)\n  &gt;&gt;&gt; pred = clf.predict(vectors_test)\n  &gt;&gt;&gt; metrics.f1_score(pred, newsgroups_test.target, average=&#39;macro&#39;)  # doctest: +ELLIPSIS\n  0.77310...\n\nThis classifier lost over a lot of its F-score, just because we removed\nmetadata that has little to do with topic classification.\nIt loses even more if we also strip this metadata from the training data:\n\n  &gt;&gt;&gt; newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;,\n  ...                                       remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;),\n  ...                                       categories=categories)\n  &gt;&gt;&gt; vectors = vectorizer.fit_transform(newsgroups_train.data)\n  &gt;&gt;&gt; clf = MultinomialNB(alpha=.01)\n  &gt;&gt;&gt; clf.fit(vectors, newsgroups_train.target)\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n  &gt;&gt;&gt; vectors_test = vectorizer.transform(newsgroups_test.data)\n  &gt;&gt;&gt; pred = clf.predict(vectors_test)\n  &gt;&gt;&gt; metrics.f1_score(newsgroups_test.target, pred, average=&#39;macro&#39;)  # doctest: +ELLIPSIS\n  0.76995...\n\nSome other classifiers cope better with this harder version of the task. Try\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\nthe ``--filter`` option to compare the results.\n\n.. topic:: Recommendation\n\n  When evaluating text classifiers on the 20 Newsgroups data, you\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n  setting ``remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)``. The F-score will be\n  lower because it is more realistic.\n\n.. topic:: Examples\n\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n\n"
    }
   ],
   "source": [
    "print(fetch_20_train.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&quot;From: lerxst@wam.umd.edu (where&#39;s my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n&quot;,\n &quot;From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven&#39;t answered this\\npoll. Thanks.\\n\\nGuy Kuo &lt;guykuo@u.washington.edu&gt;\\n&quot;]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "fetch_20_train.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[&#39;alt.atheism&#39;,\n &#39;comp.graphics&#39;,\n &#39;comp.os.ms-windows.misc&#39;,\n &#39;comp.sys.ibm.pc.hardware&#39;,\n &#39;comp.sys.mac.hardware&#39;,\n &#39;comp.windows.x&#39;,\n &#39;misc.forsale&#39;,\n &#39;rec.autos&#39;,\n &#39;rec.motorcycles&#39;,\n &#39;rec.sport.baseball&#39;,\n &#39;rec.sport.hockey&#39;,\n &#39;sci.crypt&#39;,\n &#39;sci.electronics&#39;,\n &#39;sci.med&#39;,\n &#39;sci.space&#39;,\n &#39;soc.religion.christian&#39;,\n &#39;talk.politics.guns&#39;,\n &#39;talk.politics.mideast&#39;,\n &#39;talk.politics.misc&#39;,\n &#39;talk.religion.misc&#39;]"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "fetch_20_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_dataset = load_digits(n_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;])"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "digits_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ".. _digits_dataset:\n\nOptical recognition of handwritten digits dataset\n--------------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 5620\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin &#39;@&#39; boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\n.. topic:: References\n\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n"
    }
   ],
   "source": [
    "print(digits_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "&lt;Figure size 432x288 with 1 Axes&gt;",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 245.2025 248.518125\" width=\"245.2025pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 245.2025 248.518125 \nL 245.2025 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 20.5625 224.64 \nL 238.0025 224.64 \nL 238.0025 7.2 \nL 20.5625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pb2374ca8dc)\">\n    <image height=\"218\" id=\"image91ce783e73\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"20.5625\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAAAxxJREFUeJzt3KGRIlEYRtFlawxFCCSAIQEsBkkMOCx5EAOaHNoTCbotrjeIqbm91XNOAt9vbj33VtM0TX/4tsvlkm09Ho9s63a7ZVv3+z3bqv2d+wD4DYQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGgdVSvwT/fD7p3nq9TvcqwzBkW8fjMduqedEgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAh8zX0A/7fdbjf3CYvgRYOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CCQfgn+fD6zrev1mm0t2WazmfuERfCiQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEFhN0zTNfcRPeL/f6d5+v8+2xnHMtoZhyLaOx2O2VfOiQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBL7mPuCnbLfbdO98Pmdbj8cj23q9XtmWv/eBbxEaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBBb7JXhtHMds63A4ZFun0ynbWjIvGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBoF/ed0xDF/042oAAAAASUVORK5CYII=\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0c455484c3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.1525\" xlink:href=\"#m0c455484c3\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(30.97125 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"88.5125\" xlink:href=\"#m0c455484c3\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(85.33125 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"142.8725\" xlink:href=\"#m0c455484c3\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(139.69125 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"197.2325\" xlink:href=\"#m0c455484c3\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(194.05125 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma9958acda1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"20.79\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <g transform=\"translate(7.2 24.589219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"47.97\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 51.769219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"75.15\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2 -->\n      <g transform=\"translate(7.2 78.949219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"102.33\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 106.129219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"129.51\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 4 -->\n      <g transform=\"translate(7.2 133.309219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"156.69\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 160.489219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"183.87\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 6 -->\n      <g transform=\"translate(7.2 187.669219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#ma9958acda1\" y=\"211.05\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 7 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 214.849219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 20.5625 224.64 \nL 20.5625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 238.0025 224.64 \nL 238.0025 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 20.5625 224.64 \nL 238.0025 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 20.5625 7.2 \nL 238.0025 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb2374ca8dc\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"20.5625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(digits_dataset.images[1],cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}